{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRequirements:\\n\\ntensorflow >= 1.15.0\\nnumpy\\npandas\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Requirements:\n",
    "\n",
    "tensorflow >= 1.15.0\n",
    "numpy\n",
    "pandas\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "###################################### Data Preparation ######################################\n",
    "class DataLoader(object):\n",
    "    def __init__(self, filename):\n",
    "        \n",
    "        '''\n",
    "           Initiates a data class\n",
    "              max_len : max length of the sequence\n",
    "              tag2indx : mapping from tags to indices\n",
    "              sentences: sequences\n",
    "              labels: labels of tags\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.data = pd.read_csv(filename, header = None, delimiter = \"\\t\")\n",
    "        self.data = self.data.rename(columns = {0: \"Word\", 1: \"Tag\"})\n",
    "        self.tags =  np.unique(self.data['Tag'].to_list())\n",
    "        self.data = self.data.fillna(\"END\")\n",
    "        self.pos=1\n",
    "        self.max_len = 50\n",
    "        self.sentences =[]\n",
    "        self.labels = []\n",
    "        self.tags2index = {t:i for i, t in enumerate(self.tags)}\n",
    "        tempx = []\n",
    "        tempy = []\n",
    "        \n",
    "        '''\n",
    "        \n",
    "           combining words into sentences and padding sentences to be equal to max_len\n",
    "           populating self.sentences and self.labels\n",
    "           \n",
    "        '''\n",
    "        for w,t in self.data.values:\n",
    "            if w==\"END\" and t==\"END\":\n",
    "                if len(tempx) <self.max_len:\n",
    "                   tempx  += [\"PADword\"]*(self.max_len - len(tempx))\n",
    "                   tempy  += [self.tags2index[\"O\"]]*(self.max_len - len(tempy))\n",
    "                    \n",
    "                self.sentences.append(tempx[:self.max_len])\n",
    "                self.labels.append(tempy[:self.max_len])\n",
    "                tempx =[]\n",
    "                tempy =[]\n",
    "            elif w!=\"END\" and t!=\"END\":\n",
    "                tempx.append(w)\n",
    "                tempy.append(self.tags2index[t])\n",
    "        del self.data\n",
    "\n",
    "    \n",
    "    def plot_histogram(self):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "           plot histogram fo length of all sentences\n",
    "           i.e helpful for deciding appropriate max_len value\n",
    "        '''\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.hist([ len(d) for d in self.sentences])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def space_remove(x):\n",
    "    del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Sentences: 2162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "        \n",
    "\n",
    "d = DataLoader(\"twitter_ritter.txt\")\n",
    "\n",
    "plt.figure()\n",
    "d.plot_histogram()\n",
    "\n",
    "X , Y = np.matrix(d.sentences) , np.matrix(d.labels)\n",
    "\n",
    "n_sentences = X.shape[0]\n",
    "print(\"Total Number of Sentences: {}\".format(n_sentences))\n",
    "\n",
    "'''\n",
    "  Train Test Split\n",
    "      80%  training data\n",
    "      20%  testing data\n",
    "      \n",
    "'''\n",
    "split = int(0.8*n_sentences)\n",
    "\n",
    "Xtr, Xts, Ytr, Yts = X[:split, ...], X[split:, ...],  Y[:split, ...], Y[split:, ...]\n",
    "\n",
    "'''\n",
    "   Free space on the ram\n",
    "\n",
    "'''\n",
    "space_remove(X)\n",
    "space_remove(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Set Batch Size ###################\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "######################### Elmo Embedding function ##############\n",
    "\n",
    "def ElmoEmbedding(x):\n",
    "    \n",
    "    '''\n",
    "        Returns Elmo Embedding corresponding to each word in the sentence\n",
    "         input:\n",
    "             sentence\n",
    "         output: \n",
    "            embedding of each word of the sentence\n",
    "    '''\n",
    "    return elmo_model(inputs={\"tokens\": tf.squeeze(tf.cast(x,    tf.string)),\"sequence_len\": tf.constant(batch_size*[d.max_len])\n",
    "                     },\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda, Flatten\n",
    "\n",
    "\n",
    "################################ Model Initialization ##########################################################3\n",
    "'''\n",
    "   Model Construction\n",
    "   \n",
    "'''\n",
    "\n",
    "\n",
    "class NRE():\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "             Initializes model for NER\n",
    "                 model : instance of constructed model\n",
    "        '''\n",
    "        \n",
    "        input_text = Input(shape=(d.max_len,), dtype=tf.string)\n",
    "\n",
    "        ############ Embedding Layer for Elmo Embedding  ##########\n",
    "\n",
    "        embedding = Lambda(ElmoEmbedding, output_shape=(d.max_len, 1024))(input_text)\n",
    "\n",
    "         ############ Bi Directional LSTM for tackling flows from both directions ##########\n",
    "\n",
    "        x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                               recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
    "         ############ Bi Directional LSTM for tackling flows from both directions ##########\n",
    "\n",
    "        x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                                   recurrent_dropout=0.2, dropout=0.2))(x)\n",
    "        ############ Skip Connectional for residual Learning ##########\n",
    "\n",
    "        x = add([x, x_rnn])  \n",
    "\n",
    "        out = TimeDistributed(Dense(len(d.tags), activation=\"softmax\"))(x)\n",
    "\n",
    "        self.model = Model(input_text, out)\n",
    "\n",
    "        self.model.summary()\n",
    "    def compile_model(self, opt = \"adam\", loss = \"sparse_categorical_crossentropy\"):\n",
    "        \n",
    "\n",
    "        '''\n",
    "            Compilation of Model\n",
    "                optimizer = \"adam\"\n",
    "                loss = \"Sparse Categorical Cross Entropy\"\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        self.model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "        \n",
    "    def train_model(self, Xtr, Ytr, Xval, Yval, batch_size =2 , epoch = 2):\n",
    "        \n",
    "        '''\n",
    "            Train Model\n",
    "                Xtr, Ytr : training data\n",
    "                Yval, Yval : Validation data\n",
    "                batch_size = 2 (default)\n",
    "                epoch = 2 (default)\n",
    "                \n",
    "        \n",
    "        '''\n",
    "        history = self.model.fit(Xtr, Ytr, validation_data=(Xval[:-1], Yval[:-1]), batch_size=batch_size, epochs=epoch, verbose=1)\n",
    "        return history\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rimsh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rimsh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 50, 1024)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 50, 1024)     6295552     lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 50, 1024)     6295552     bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 50, 1024)     0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 50, 22)       22550       add_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 12,613,654\n",
      "Trainable params: 12,613,654\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nre = NRE()\n",
    "nre.compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr shape:  (1556, 50)\n",
      "Y_tr shape:  (1556, 50)\n"
     ]
    }
   ],
   "source": [
    "val_split = int(0.9 * Xtr.shape[0])\n",
    "\n",
    "X_tr, X_val = Xtr[:val_split], Xtr[val_split:]\n",
    "\n",
    "y_tr, y_val = Ytr[:val_split], Ytr[val_split: ]\n",
    "\n",
    "\n",
    "############################ Space Efficiency ##########\n",
    "\n",
    "space_remove(Xtr)\n",
    "space_remove(Ytr)\n",
    "\n",
    "print(\"X_tr shape: \", X_tr.shape)\n",
    "print(\"Y_tr shape: \",y_tr.shape)\n",
    "\n",
    "\n",
    "y_tr , y_val = y_tr[..., None], y_val[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rimsh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rimsh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rimsh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rimsh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rimsh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rimsh\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1556 samples, validate on 172 samples\n",
      "Epoch 1/2\n",
      " 576/1556 [==========>...................] - ETA: 25:24 - loss: 0.1094 - accuracy: 0.9783"
     ]
    }
   ],
   "source": [
    "############################# Model Training ##########################################33\n",
    "train = True\n",
    "if train:\n",
    "    hist = nre.train_model(X_tr, y_tr, X_val, y_val)\n",
    "    nre.model.save_weights(\"nre.h5\")\n",
    "else:\n",
    "    nre.model.load_weights(\"nre\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Evaluate Model on Test dataset\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "acc = nre.model.evaluate(Xts[:-1], Yts[:-1][..., None], batch_size = 2)[1]\n",
    "print(\"Test Accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize \n",
    "idx2tags = {i:t for t, i in d.tags2index.items()}\n",
    "\n",
    "class NER_tester():\n",
    "    \n",
    "    \n",
    "    def __init__(self, text):\n",
    "        '''\n",
    "            input:\n",
    "                text : text string to be tested\n",
    "            Output:\n",
    "                print predicted entities\n",
    "        '''\n",
    "        \n",
    "        d_text = self.construct_test(test)\n",
    "        ############### predict labels ###############\n",
    "        pred_labels  = nre.model.predict(d_text)\n",
    "        \n",
    "        self.indices2tags(d_text, pred_labels)\n",
    "        \n",
    "    def construct_test(self, test):\n",
    "        \n",
    "        '''\n",
    "        prepare test  string data for  model\n",
    "        input:\n",
    "          test string\n",
    "        outpur:\n",
    "          preprocessed string\n",
    "        '''\n",
    "\n",
    "\n",
    "        test = word_tokenize(test)\n",
    "\n",
    "        test =[t for t in test if t.isalpha()]\n",
    "        \n",
    "        ################## assert length of sentence is less than max_len #################\n",
    "        assert len(test)<= d.max_len\n",
    "\n",
    "        #################### Pad array to be equal to batch size for elmo embedding ############### \n",
    "        \n",
    "        test += [\"PADword\"]*(d.max_len -  len(test))\n",
    "        \n",
    "\n",
    "        return np.concatenate([np.array(test)[None, :], np.array([\"PADword\"]*d.max_len)[None, :]], axis = 0)\n",
    "    \n",
    "    def indices2tags(self, text, preds):\n",
    "        '''\n",
    "        Convert model prediction into tags and print predict entities\n",
    "\n",
    "        input:\n",
    "            text: test string\n",
    "            preds: model prediction\n",
    "        output:\n",
    "            print import entities and probabilities\n",
    "\n",
    "        '''\n",
    "\n",
    "        assert text.shape[0]>1\n",
    "        ############ Only first sample is useful, strip away padded information #################\n",
    "        \n",
    "        text = text[0]\n",
    "        preds = preds[0]\n",
    "        \n",
    "        for i, p in enumerate(preds):\n",
    "            label = idx2tags[np.argmax(p)]\n",
    "\n",
    "            actual = \"\"\n",
    "            \n",
    "            ################  Print only identified entities ###########\n",
    "         \n",
    "            if label != \"O\":\n",
    "\n",
    "               sort_labels = [(idx2tags[k], prob)  for k, prob in enumerate(p)]\n",
    "               \n",
    "               ################ Sort Lables according to Probability ###########\n",
    "\n",
    "               sort_labels.sort(key = lambda x: x[1], reverse = True)\n",
    "\n",
    "               for t, prob in sort_labels:\n",
    "                    \n",
    "                    ################## For all labels format upto 2 decimal for peobability ###########\n",
    "                    if t != \"O\":\n",
    "                        actual +=  t + \" {:.02f}\".format(prob) + \"| \"   \n",
    "\n",
    "               print(text[i])\n",
    "               print(label)\n",
    "               print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA\n",
      "B-geo-loc\n",
      "B-geo-loc 0.96| B-company 0.01| B-facility 0.01| I-geo-loc 0.00| B-person 0.00| B-other 0.00| B-musicartist 0.00| B-product 0.00| I-other 0.00| B-sportsteam 0.00| I-facility 0.00| I-person 0.00| B-movie 0.00| B-tvshow 0.00| I-company 0.00| I-sportsteam 0.00| I-product 0.00| I-musicartist 0.00| I-movie 0.00| I-tvshow 0.00| nan 0.00| \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.NER_tester at 0x2545abe2eb8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################### Please Enter Input String of length less than 50 ##############################\n",
    "\n",
    "test = \"I live in USA.\"\n",
    "\n",
    "NER_tester(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
